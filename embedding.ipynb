{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import statistics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# load mean and stddev\n",
    "with open('feat_dict.pkl', 'rb') as file:\n",
    "    values = pickle.load(file)\n",
    "\n",
    "def dataAug(index_list):\n",
    "    aug_features = []\n",
    "    for index in index_list:\n",
    "        ori_start_index = int(index.item())\n",
    "        ori_label = values[ori_start_index]['label']\n",
    "        \n",
    "        random_number = random.randint(1, 10)\n",
    "        start_index = ori_start_index + random_number\n",
    "        \n",
    "        if start_index in values:\n",
    "            start_label = values[start_index]['label']\n",
    "            if start_label != ori_label:\n",
    "                start_index = ori_start_index - random_number\n",
    "                start_label = values[start_index]['label']\n",
    "        else:\n",
    "            start_index = ori_start_index - random_number\n",
    "            start_label = values[start_index]['label']\n",
    "        \n",
    "        aug_feat = values[start_index]['feature']\n",
    "        aug_features.append(aug_feat)\n",
    "    return aug_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, feat_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            input_dim = hidden_dim\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        return feat\n",
    "    \n",
    "\n",
    "class Proj(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, feat_dim):\n",
    "        super(Proj, self).__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(input_dim, feat_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        contrastive_feat = F.normalize(self.head(x), dim=1)\n",
    "\n",
    "        return contrastive_feat\n",
    "    \n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from losses import SupConLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from sklearn.metrics import average_precision_score\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 从文件中读取均值和标准差\n",
    "with open('scaler_params.pkl', 'rb') as file:\n",
    "    mean_cpu, std_cpu = pickle.load(file)\n",
    "mean = torch.tensor(mean_cpu).to(device)\n",
    "std = torch.tensor(std_cpu).to(device)\n",
    "performances = []\n",
    "cls_losses = []\n",
    "con_losses = []\n",
    "acc_list= []\n",
    "\n",
    "def train(data_loader, time, host_list, criterion, optimizer, device, epochs, save_path, Lambda = 0):\n",
    "\n",
    "    encoder.train()\n",
    "    projector.train()\n",
    "    classification_model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss1 = 0\n",
    "        total_loss2 = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for batch_features, labels in progress_bar:\n",
    "            batch_features = batch_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            features1 = batch_features[:,:-1]\n",
    "            aug_index = batch_features[:,-1:]\n",
    "            \n",
    "            # data augmentation\n",
    "            aug_features = dataAug(aug_index)\n",
    "            aug_features = torch.tensor(aug_features, dtype=torch.float32)\n",
    "            aug_features = aug_features.to(device)\n",
    "            aug_features = (aug_features - mean) / std\n",
    "\n",
    "            bsz = labels.shape[0]\n",
    "            aug_input = torch.cat([features1, aug_features], dim=0)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            embs = encoder(aug_input)\n",
    "            embs_to_predict = embs[:bsz,:]\n",
    "            contrastive_feat = projector(embs)\n",
    "            predictions = classification_model(embs_to_predict)\n",
    "            \n",
    "            f1, f2 = torch.split(contrastive_feat, [bsz, bsz], dim=0)\n",
    "            features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "            \n",
    "            loss1 = criterion(features = features, device = device , labels = labels)\n",
    "            loss2 = criterion2(predictions , labels)\n",
    "            loss = loss2 + Lambda*loss1\n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "            \n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss1 += loss1.item()\n",
    "            total_loss2 += loss2.item()\n",
    "            progress_bar.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        average_loss1 = total_loss1 / len(train_dataloader)\n",
    "        average_loss2 = total_loss2 / len(train_dataloader)\n",
    "        accuracy = correct / total_samples\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] CLS_loss: {average_loss2:.4f} Con_loss: {average_loss1:.4f} Accuracy: {accuracy:.4f}\")\n",
    "        cls_losses.append(average_loss2)\n",
    "        con_losses.append(average_loss1)\n",
    "        acc_list.append(accuracy)\n",
    "        \n",
    "        # evaluation for test dataset\n",
    "        classification_model.eval()  \n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []  \n",
    "\n",
    "        with torch.no_grad():  \n",
    "            \n",
    "            for test_data, test_labels in test_dataloader:\n",
    "                test_data = test_data.to(device)\n",
    "                test_labels = test_labels.to(device)\n",
    "\n",
    "                test_features = test_data[:,:-1]\n",
    "\n",
    "                encoder_output = encoder(test_features)\n",
    "                predictions = classification_model(encoder_output)\n",
    "\n",
    "                loss = criterion2(predictions, test_labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(predictions, 1)\n",
    "                all_labels.extend(test_labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                \n",
    "                probs = torch.softmax(predictions, dim=1)\n",
    "                all_probs.append(probs.cpu().numpy()) \n",
    "        \n",
    "        all_probs = np.concatenate(all_probs)\n",
    "        all_labels_one_hot = np.eye(predictions.shape[1])[all_labels] \n",
    "        mAP = average_precision_score(all_labels_one_hot, all_probs, average=\"macro\")\n",
    "\n",
    "        print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall = recall_score(all_labels, all_preds, average='macro')\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "        classification_model.train()\n",
    "        \n",
    "        performances.append(round(mAP, 4))\n",
    "        \n",
    "    print(performances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import statistics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "encoder = Encoder(input_dim=10, hidden_dims=[10, 16, 32, 16], feat_dim=4).to(device)\n",
    "projector = Proj(input_dim=16, hidden_dims=[10, 16], feat_dim=4).to(device)\n",
    "\n",
    "classification_model = ClassificationHead(input_dim=16, num_classes=7)\n",
    "classification_model.to(device)\n",
    "\n",
    "# start training\n",
    "time = 1\n",
    "host_list = ['223.5.5.5', '8.8.8.8']\n",
    "\n",
    "criterion = SupConLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, initial_lr=0.01, step_size=0.01, max_lr=0.5):\n",
    "    new_lr = initial_lr + (epoch * step_size)\n",
    "    new_lr = min(new_lr, max_lr)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "                {'params': encoder.parameters()},\n",
    "                {'params': classification_model.parameters()},\n",
    "                {'params': projector.parameters()},\n",
    "            ], lr = 0.001)\n",
    "\n",
    "train(train_dataloader, time, host_list, criterion, optimizer, device, epochs=100, save_path='checkpoints_new/',Lambda = 0.25)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
